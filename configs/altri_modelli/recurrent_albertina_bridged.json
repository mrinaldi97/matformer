{
    "model_class": "Autoregressive_Model",
    "model_config": {
      "name": "Albertina_Recurrent",
      "hidden_size": 768,
      "ffn_factor": 4.0,
      "vocab_size": 32768,
      "bos_token_id": 5,
      "eos_token_id": 6,
      "pad_token_id": 0,
      "mask_token_id": 4,
      "num_hidden_layers": 12,
      "num_attention_heads": 12,
      "tie_word_embeddings": false,
      "rms_norm_eps": 1e-6,
      "attention_type": [],
      "max_position_embeddings": 1024,
      "block_size_for_attention": 128,
      "compile_flexattn": false,
      "bias": false,
      "training_objective": "autoregressive",
      "is_causal": true,
      "default_layer": {
        "attn_impl": "flash",
        "sliding_window_size": null,
        "positional_encoding": "alibi",
        "normalization":"rmsnorm",
        "normalization_position":"pre",
        "ffn_activation":"swiglu",
        "hooks": {}
      },
      "custom_layers": {
          "9": {
            "hooks": {
              "before_output": {"type": "previous_state_saver", "detach_depth": 1}
            }
          },
          "10": {
            "hooks": {
              "before_output": {"type": "previous_state_saver", "detach_depth": 1}
            }
          },
          "1": {
            "hooks": {
              "pre_mlp": {"type": "recurrence_bridge_injector", "receive_from": 9}
            }
          },       
          "2": {
            "hooks": {
              "pre_mlp": {"type": "recurrence_bridge_injector", "receive_from": 9}
            }
           },
          "3": {
            "hooks": {
              "pre_mlp": {"type": "recurrence_bridge_injector", "receive_from": 10}
            }
          },
          "4": {
            "hooks": {
              "pre_mlp": {"type": "recurrence_bridge_injector", "receive_from": 10}
            }
          }
       }
  },
  "training": {
        "optimizer": "muon",
        "lr_scheduling": true,
        "lr": 5e-4,
        "final_lr":2e-5,
        "hold_steps":0.21,
        "weight_decay": 0.01,
        "scheduler": "custom",
        "gradient_clip_val":1.0,
        "warmup_steps": 0.05,
        "max_epochs": 1,
        "accumulate_grad_batches": 16,
        "seed": 27,
        "save_every_n_steps": 5000,
        "checkpoint_name": "albertina_Recurrent"
    },
  "tokenizer": {
    "type": "huggingface",
    "pretrained_name": "mrinaldi/Gettone",
    "varlen_strategy": "unpadding"
  },
  "data": {
    "data_root": "/home/matteo/Albertone/Albertina/mini-albertina-2",
    "batch_size": 36,
    "num_workers": 1,
    "mdat_strategy":"Gettone_1024",
    "mdat_view": null,
    "wanted_from_strategy":"chunked_for_recurrence"
  },
  "save_dir": "./checkpoints_ablations",
  "wandb_project": "Albertina_Ablation_Studies",
  "wandb_run_name": "Albertina_Autoregressive_Recurrent"
}
