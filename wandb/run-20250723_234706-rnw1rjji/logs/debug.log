2025-07-23 23:47:06,134 INFO    MainThread:5729 [wandb_setup.py:_flush():68] Current SDK version is 0.19.10
2025-07-23 23:47:06,134 INFO    MainThread:5729 [wandb_setup.py:_flush():68] Configure stats pid to 5729
2025-07-23 23:47:06,134 INFO    MainThread:5729 [wandb_setup.py:_flush():68] Loading settings from /home/matteo/.config/wandb/settings
2025-07-23 23:47:06,134 INFO    MainThread:5729 [wandb_setup.py:_flush():68] Loading settings from /home/matteo/Ricerca/miei_progetti/matformer/wandb/settings
2025-07-23 23:47:06,134 INFO    MainThread:5729 [wandb_setup.py:_flush():68] Loading settings from environment variables
2025-07-23 23:47:06,135 INFO    MainThread:5729 [wandb_init.py:setup_run_log_directory():724] Logging user logs to ./wandb/run-20250723_234706-rnw1rjji/logs/debug.log
2025-07-23 23:47:06,135 INFO    MainThread:5729 [wandb_init.py:setup_run_log_directory():725] Logging internal logs to ./wandb/run-20250723_234706-rnw1rjji/logs/debug-internal.log
2025-07-23 23:47:06,135 INFO    MainThread:5729 [wandb_init.py:init():852] calling init triggers
2025-07-23 23:47:06,135 INFO    MainThread:5729 [wandb_init.py:init():857] wandb.init called with sweep_config: {}
config: {'model_class': 'BERTModel', 'model_config': {'name': 'Medio-Albertino Muon', 'hidden_size': 768, 'ffn_factor': 1.0, 'num_hidden_layers': 24, 'num_attention_heads': 12, 'vocab_size': 32768, 'bos_token_id': 1, 'eos_token_id': 2, 'pad_token_id': 0, 'tie_word_embeddings': False, 'rms_norm_eps': 1e-06, 'attention_type': ['sliding'], 'sliding_window_size': 512, 'sliding_layers': [], 'sliding_type': 'disabled', 'max_position_embeddings': 1024, 'block_size_for_attention': 128, 'compile_flexattn': False, 'bias': False, 'training_objective': 'masked', 'alibi': True, 'is_causal': False, 'attn_impl': 'flash'}, 'training': {'optimizer': 'muon', 'muon_lr': 0.01, 'lr': 0.0001, 'weight_decay': 0.01, 'max_steps': 100000000000000000, 'accumulate_grad_batches': 1, 'seed': 27, 'checkpoint_name': 'medio_albertino'}, 'tokenizer': {'type': 'huggingface', 'pretrained_name': 'sapienzanlp/Minerva-350M-base-v1.0', 'varlen_strategy': 'unpadding'}, 'data': {'data_root': '../matformer_norepo/liberliber_1024_tokens', 'batch_size': 20, 'num_workers': 2}, 'save_dir': './checkpoints', 'wandb_project': 'matformer', 'wandb_run_name': 'medio-albertino-muon-liberliber-1024-tokens', '_wandb': {}}
2025-07-23 23:47:06,135 INFO    MainThread:5729 [wandb_init.py:init():893] starting backend
2025-07-23 23:47:06,136 INFO    MainThread:5729 [wandb_init.py:init():897] sending inform_init request
2025-07-23 23:47:06,141 INFO    MainThread:5729 [backend.py:_multiprocessing_setup():101] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
2025-07-23 23:47:06,141 INFO    MainThread:5729 [wandb_init.py:init():907] backend started and connected
2025-07-23 23:47:06,145 INFO    MainThread:5729 [wandb_init.py:init():1002] updated telemetry
2025-07-23 23:47:06,152 INFO    MainThread:5729 [wandb_init.py:init():1026] communicating run to backend with 90.0 second timeout
2025-07-23 23:47:07,104 INFO    MainThread:5729 [wandb_init.py:init():1101] starting run threads in backend
2025-07-23 23:47:07,687 INFO    MainThread:5729 [wandb_run.py:_console_start():2566] atexit reg
2025-07-23 23:47:07,688 INFO    MainThread:5729 [wandb_run.py:_redirect():2414] redirect: wrap_raw
2025-07-23 23:47:07,688 INFO    MainThread:5729 [wandb_run.py:_redirect():2483] Wrapping output streams.
2025-07-23 23:47:07,688 INFO    MainThread:5729 [wandb_run.py:_redirect():2506] Redirects installed.
2025-07-23 23:47:07,691 INFO    MainThread:5729 [wandb_init.py:init():1147] run started, returning control to user process
2025-07-23 23:47:12,623 INFO    MainThread:5729 [wandb_run.py:_config_callback():1429] config_cb None None {'ModelClass': 'BERTModel', 'config': "ModelConfig(hidden_size=768, ffn_factor=1.0, num_hidden_layers=24, num_attention_heads=12, vocab_size=32768, pad_token_id=0, bos_token_id=1, eos_token_id=2, tie_word_embeddings=False, rms_norm_eps=1e-06, attention_type=['sliding'], sliding_window_size=512, sliding_layers=[], sliding_type='disabled', max_position_embeddings=1024, block_size_for_attention=128, compile_flexattn=False, bias=False, name='Medio-Albertino Muon', training_objective='masked', is_causal=False, alibi=True, attn_impl='flash')", 'tokenizer': '<matformer.tokenizers.MatformerTokenizer object at 0x7f6e3948aad0>', 'device': 'cuda', 'train_config': {'optimizer': 'muon', 'muon_lr': 0.01, 'lr': 0.0001, 'weight_decay': 0.01, 'max_steps': 100000000000000000, 'accumulate_grad_batches': 1, 'seed': 27, 'checkpoint_name': 'medio_albertino'}, 'inference_fix': False, 'nested': False}
2025-07-24 00:17:50,741 INFO    MsgRouterThr:5729 [mailbox.py:close():129] [no run ID] Closing mailbox, abandoning 1 handles.
