_wandb:
    value:
        cli_version: 0.19.10
        m:
            - "1": trainer/global_step
              "6":
                - 3
              "7": []
            - "1": train/loss
              "5": 1
              "6":
                - 1
                - 3
              "7": []
            - "1": epoch
              "5": 1
              "6":
                - 1
                - 3
              "7": []
        python_version: 3.13.5
        t:
            "1":
                - 1
                - 5
                - 9
                - 11
                - 41
                - 49
                - 53
                - 55
                - 71
                - 103
                - 105
            "2":
                - 1
                - 5
                - 9
                - 11
                - 41
                - 49
                - 53
                - 55
                - 71
                - 103
                - 105
            "3":
                - 7
                - 13
                - 16
                - 23
                - 55
                - 66
            "4": 3.13.5
            "5": 0.19.10
            "6": 4.52.0.dev0
            "8":
                - 5
            "12": 0.19.10
            "13": linux-x86_64
attn_impl:
    value: flash
config:
    value: ModelConfig(hidden_dim=512, ffn_factor=1.0, n_layers=10, n_heads=8, vocab_size=260, pad_id=258, bos_id=256, eos_id=257, tie_word_embeddings=False, rms_norm_eps=1e-06, attention_type=['causal', 'sliding'], sliding_window_size=512, sliding_layers=[0, 2, 4, 6, 8], sliding_type='partial', max_seqlen=1024, block_size_for_attention=128, compile_flexattn=False, bias=False, name='EntropySmall', training_objective='autoregressive', is_causal=True, alibi=True, attn_impl='flash')
data:
    value:
        batch_size: 64
        data_root: ../matformer_norepo/liberliber_1024
        num_workers: 4
device:
    value: cuda
inference_fix:
    value: false
model_class:
    value: EntropyModel
model_config:
    value:
        alibi: true
        attention_type:
            - causal
            - sliding
        attn_impl: flash
        bias: false
        block_size_for_attention: 128
        bos_id: 256
        compile_flexattn: false
        eos_id: 257
        ffn_factor: 1
        hidden_dim: 512
        is_causal: true
        max_seqlen: 1024
        n_heads: 8
        n_layers: 10
        name: EntropySmall
        pad_id: 258
        rms_norm_eps: 1e-06
        sliding_layers:
            - 0
            - 2
            - 4
            - 6
            - 8
        sliding_type: partial
        sliding_window_size: 512
        tie_word_embeddings: false
        training_objective: autoregressive
        vocab_size: 260
nested:
    value: false
save_dir:
    value: ./checkpoints
tokenizer:
    value:
        type: bytes
        varlen_strategy: unpadding
train_config:
    value:
        accumulate_grad_batches: 1
        checkpoint_name: entropy_small
        lr: 0.0001
        max_steps: 100000
        seed: 27
training:
    value:
        accumulate_grad_batches: 1
        checkpoint_name: entropy_small
        lr: 0.0001
        max_steps: 100000
        seed: 27
wandb_project:
    value: matformer
wandb_run_name:
    value: entropy-small-liberliber-1024-bytes
